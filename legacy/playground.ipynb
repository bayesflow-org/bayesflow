{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.contrib.eager as tfe\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.tri as tri\n",
    "from scipy import stats\n",
    "from scipy.special import gamma as gamma_fun\n",
    "import scipy.special as spec\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "code_folding": [
     179,
     282,
     342,
     386
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "class Permutation(tf.keras.Model):\n",
    "    \"\"\"Implements a permutation layer to permute the input dimensions of the cINN block.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        \"\"\"\n",
    "        Creates a permutation layer for a conditional invertible block.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        input_dim  : int -- the dimensionality of the input to the c inv block.\n",
    "        \"\"\"\n",
    "\n",
    "        super(Permutation, self).__init__()\n",
    "\n",
    "        permutation_vec = np.random.permutation(input_dim)\n",
    "        inv_permutation_vec = np.argsort(permutation_vec)\n",
    "        self.permutation = tf.Variable(initial_value=permutation_vec,\n",
    "                                       trainable=False,\n",
    "                                       dtype=tf.int32,\n",
    "                                       name='permutation')\n",
    "        self.inv_permutation = tf.Variable(initial_value=inv_permutation_vec,\n",
    "                                           trainable=False,\n",
    "                                           dtype=tf.int32,\n",
    "                                           name='inv_permutation')\n",
    "\n",
    "    def call(self, x, inverse=False):\n",
    "        \"\"\"Permutes the bach of an input.\"\"\"\n",
    "\n",
    "        if not inverse:\n",
    "            return tf.transpose(tf.gather(tf.transpose(x), self.permutation))\n",
    "        return tf.transpose(tf.gather(tf.transpose(x), self.inv_permutation))\n",
    "\n",
    "\n",
    "class CouplingNet(tf.keras.Model):\n",
    "    \"\"\"Implements a conditional version of a sequential network.\"\"\"\n",
    "\n",
    "    def __init__(self, meta, n_out):\n",
    "        \"\"\"\n",
    "        Creates a conditional coupling net (FC neural network).\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        meta  : list -- a list of dictionaries, wherein each dictionary holds parameter - value pairs for a single\n",
    "                       tf.keras.Dense layer.\n",
    "        n_out : int  -- number of outputs of the coupling net\n",
    "        \"\"\"\n",
    "\n",
    "        super(CouplingNet, self).__init__()\n",
    "\n",
    "        self.dense = tf.keras.Sequential(\n",
    "            # Hidden layer structure\n",
    "            [tf.keras.layers.Dense(units,\n",
    "                                   activation=meta['activation'],\n",
    "                                   kernel_initializer=meta['initializer'])\n",
    "             for units in meta['n_units']] +\n",
    "            # Output layer\n",
    "            [tf.keras.layers.Dense(n_out,\n",
    "                                   kernel_initializer=meta['initializer'])]\n",
    "        )\n",
    "\n",
    "    def call(self, theta, x):\n",
    "        \"\"\"\n",
    "        Concatenates x and y and performs a forward pass through the coupling net.\n",
    "        Arguments:\n",
    "        theta : tf.Tensor of shape (batch_size, inp_dim)     -- the parameters x ~ p(x|y) of interest\n",
    "        x     : tf.Tensor of shape (batch_size, summary_dim) -- the summarized conditional data of interest y = sum(y)\n",
    "        \"\"\"\n",
    "\n",
    "        inp = tf.concat((theta, x), axis=-1)\n",
    "        out = self.dense(inp)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConditionalInvertibleBlock(tf.keras.Model):\n",
    "    \"\"\"Implements a conditional version of the INN block.\"\"\"\n",
    "\n",
    "    def __init__(self, meta):\n",
    "        \"\"\"\n",
    "        Creates a conditional invertible block.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        meta      : list -- a list of dictionaries, wherein each dictionary holds parameter - value pairs for a single\n",
    "                       tf.keras.Dense layer. All coupling nets are assumed to be equal.\n",
    "        theta_dim : int  -- the number of outputs of the invertible block (eq. the dimensionality of the latent space)\n",
    "        \"\"\"\n",
    "\n",
    "        super(ConditionalInvertibleBlock, self).__init__()\n",
    "        self.alpha = meta['alpha']\n",
    "        self.n_out1 = meta['theta_dim'] // 2\n",
    "        self.n_out2 = meta['theta_dim'] // 2 if meta['theta_dim'] % 2 == 0 else meta['theta_dim'] // 2 + 1\n",
    "        if meta['permute']:\n",
    "            self.permutation = Permutation(meta['theta_dim'])\n",
    "        else:\n",
    "            self.permutation = None\n",
    "        self.s1 = CouplingNet(meta, self.n_out1)\n",
    "        self.t1 = CouplingNet(meta, self.n_out1)\n",
    "        self.s2 = CouplingNet(meta, self.n_out2)\n",
    "        self.t2 = CouplingNet(meta, self.n_out2)\n",
    "\n",
    "    def call(self, theta, x, inverse=False, log_det_J=True):\n",
    "        \"\"\"\n",
    "        Implements both directions of a conditional invertible block.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        theta     : tf.Tensor of shape (batch_size, theta_dim) -- the parameters theta ~ p(theta|y) of interest\n",
    "        x         : tf.Tensor of shape (batch_size, summary_dim) -- the summarized conditional data of interest x = sum(x)\n",
    "        inverse   : bool -- flag indicating whether to tun the block forward or backwards\n",
    "        log_det_J : bool -- flag indicating whether to return the log determinant of the Jacobian matrix\n",
    "        ----------\n",
    "\n",
    "        Returns:\n",
    "        (v, log_det_J)  :  (tf.Tensor of shape (batch_size, inp_dim), tf.Tensor of shape (batch_size, )) --\n",
    "                           the transformed input, if inverse = False, and the corresponding Jacobian of the transformation\n",
    "                            if inverse = False\n",
    "        u               :  tf.Tensor of shape (batch_size, inp_dim) -- the transformed out, if inverse = True\n",
    "        \"\"\"\n",
    "\n",
    "        # --- Forward pass --- #\n",
    "        if not inverse:\n",
    "\n",
    "            if self.permutation is not None:\n",
    "                theta = self.permutation(theta)\n",
    "            \n",
    "            u1, u2 = tf.split(theta, [self.n_out1, self.n_out2], axis=-1)\n",
    "\n",
    "            # Pre-compute network outputs for v1\n",
    "            s1 = self.s1(u2, x)\n",
    "            # Clamp s1 if specified\n",
    "            if self.alpha is not None:\n",
    "                s1 = (2. * self.alpha / np.pi) * tf.math.atan(s1 / self.alpha)\n",
    "            t1 = self.t1(u2, x)\n",
    "            v1 = u1 * tf.exp(s1) + t1\n",
    "\n",
    "            # Pre-compute network outputs for v2\n",
    "            s2 = self.s2(v1, x)\n",
    "            # Clamp s2 if specified\n",
    "            if self.alpha is not None:\n",
    "                s2 = (2. * self.alpha / np.pi) * tf.math.atan(s2 / self.alpha)\n",
    "            t2 = self.t2(v1, x)\n",
    "            v2 = u2 * tf.exp(s2) + t2\n",
    "            v = tf.concat((v1, v2), axis=-1)\n",
    "\n",
    "            if log_det_J:\n",
    "                # log|J| = log(prod(diag(J))) -> according to inv architecture\n",
    "                return v, tf.reduce_sum(s1, axis=-1) + tf.reduce_sum(s2, axis=-1)\n",
    "            return v\n",
    "\n",
    "        # --- Inverse pass --- #\n",
    "        else:\n",
    "\n",
    "            v1, v2 = tf.split(theta, [self.n_out1, self.n_out2], axis=-1)\n",
    "\n",
    "            # Pre-Compute s2\n",
    "            s2 = self.s2(v1, x)\n",
    "            # Clamp s2 if specified\n",
    "            if self.alpha is not None:\n",
    "                s2 = (2. * self.alpha / np.pi) * tf.math.atan(s2 / self.alpha)\n",
    "            u2 = (v2 - self.t2(v1, x)) * tf.exp(-s2)\n",
    "\n",
    "            # Pre-Compute s1\n",
    "            s1 = self.s1(u2, x)\n",
    "            # Clamp s1 if specified\n",
    "            if self.alpha is not None:\n",
    "                s1 = (2. * self.alpha / np.pi) * tf.math.atan(s1 / self.alpha)\n",
    "            u1 = (v1 - self.t1(u2, x)) * tf.exp(-s1)\n",
    "            u = tf.concat((u1, u2), axis=-1)\n",
    "\n",
    "            if self.permutation is not None:\n",
    "                u = self.permutation(u, inverse=True)\n",
    "            return u\n",
    "\n",
    "\n",
    "class BayesFlow(tf.keras.Model):\n",
    "    \"\"\"Implements a chain of conditional invertible blocks for Bayesian parameter inference.\"\"\"\n",
    "\n",
    "    def __init__(self, meta, n_blocks, theta_dim, alpha=1.9, summary_net=None, permute=False):\n",
    "        \"\"\"\n",
    "        Creates a chain of cINN blocks and chains operations.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        meta        : list -- a list of dictionary, where each dictionary holds parameter - value pairs for a single\n",
    "                                  keras.Dense layer\n",
    "        n_blocks    : int  -- the number of invertible blocks\n",
    "        theta_dim   : int  -- the dimensionality of the parameter space to be learned\n",
    "        summary_net : tf.keras.Model or None -- an optinal summary network for learning the sumstats of x\n",
    "        permute     : bool -- whether to permute the inputs to the cINN\n",
    "        \"\"\"\n",
    "\n",
    "        super(BayesFlow, self).__init__()\n",
    "\n",
    "        self.cINNs = [ConditionalInvertibleBlock(meta, theta_dim, alpha=alpha, permute=permute) for _ in range(n_blocks)]\n",
    "        self.summary_net = summary_net\n",
    "        self.theta_dim = theta_dim\n",
    "\n",
    "    def call(self, theta, x, inverse=False):\n",
    "        \"\"\"\n",
    "        Performs one pass through an invertible chain (either inverse or forward).\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        theta     : tf.Tensor of shape (batch_size, inp_dim) -- the parameters theta ~ p(theta|x) of interest\n",
    "        x         : tf.Tensor of shape (batch_size, summary_dim) -- the conditional data x\n",
    "        inverse   : bool -- flag indicating whether to tun the chain forward or backwards\n",
    "        ----------\n",
    "\n",
    "        Returns:\n",
    "        (z, log_det_J)  :  (tf.Tensor of shape (batch_size, inp_dim), tf.Tensor of shape (batch_size, )) --\n",
    "                           the transformed input, if inverse = False, and the corresponding Jacobian of the transformation\n",
    "                            if inverse = False\n",
    "        x               :  tf.Tensor of shape (batch_size, inp_dim) -- the transformed out, if inverse = True\n",
    "        \"\"\"\n",
    "\n",
    "        if self.summary_net is not None:\n",
    "            x = self.summary_net(x)\n",
    "        if inverse:\n",
    "            return self.inverse(theta, x)\n",
    "        else:\n",
    "            return self.forward(theta, x)\n",
    "\n",
    "    def forward(self, theta, x):\n",
    "        \"\"\"Performs a forward pass though the chain.\"\"\"\n",
    "\n",
    "        z = theta\n",
    "        log_det_Js = []\n",
    "        for cINN in self.cINNs:\n",
    "            z, log_det_J = cINN(z, x)\n",
    "            log_det_Js.append(log_det_J)\n",
    "        # Sum Jacobian determinants for all blocks to obtain total Jacobian.\n",
    "        log_det_J = tf.add_n(log_det_Js)\n",
    "        return {'z': z, 'log_det_J': log_det_J}\n",
    "\n",
    "    def inverse(self, z, x):\n",
    "        \"\"\"Performs a reverse pass through the chain.\"\"\"\n",
    "\n",
    "        theta = z\n",
    "        for cINN in reversed(self.cINNs):\n",
    "            theta = cINN(theta, x, inverse=True)\n",
    "        return theta\n",
    "\n",
    "    def sample(self, x, n_samples, to_numpy=False, training=False):\n",
    "        \"\"\"\n",
    "        Samples from the inverse model given a single instance y or a batch of instances.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        x         : tf.Tensor of shape (batch_size, summary_dim) -- the conditioning data of interest\n",
    "        n_samples : int -- number of samples to obtain from the approximate posterior\n",
    "        to_numpy  : bool -- flag indicating whether to return the samples as a np.array or a tf.Tensor\n",
    "        training  : bool -- flag used to indicate that samples are drawn are training time (BatchNorm)\n",
    "        ----------\n",
    "\n",
    "        Returns:\n",
    "        theta_samples : 3D tf.Tensor or np.array of shape (n_samples, n_batch, theta_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        # Summarize obs data if summary net available\n",
    "        if self.summary_net is not None:\n",
    "            x = self.summary_net(x, training=training)\n",
    "\n",
    "        # In case x is a single instance\n",
    "        if int(x.shape[0]) == 1:\n",
    "            z_normal_samples = tf.random_normal(shape=(n_samples, self.theta_dim), dtype=tf.float32)\n",
    "            theta_samples = self.inverse(z_normal_samples, tf.tile(x, [n_samples, 1]))\n",
    "        # In case of a batch input, send a 3D tensor through the invertible chain and use tensordot\n",
    "        # Warning: This tensor could get pretty big if sampling a lot of values for a lot of batch instances!\n",
    "        else:\n",
    "            z_normal_samples = tf.random_normal(shape=(n_samples, int(x.shape[0]), self.theta_dim), dtype=tf.float32)\n",
    "            theta_samples = self.inverse(z_normal_samples, tf.stack([x] * n_samples))\n",
    "\n",
    "        if to_numpy:\n",
    "            return theta_samples.numpy()\n",
    "        return theta_samples\n",
    "\n",
    "\n",
    "class InvariantModule(tf.keras.Model):\n",
    "    \"\"\"Implements an invariant nn module as proposed by Bloem-Reddy and Teh (2019).\"\"\"\n",
    "\n",
    "    def __init__(self, meta):\n",
    "        \"\"\"\n",
    "        Creates an invariant function with mean pooling.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        meta : dict -- a dictionary with hyperparameter name - values\n",
    "        \"\"\"\n",
    "\n",
    "        super(InvariantModule, self).__init__()\n",
    "\n",
    "\n",
    "        self.module = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(**meta['dense_inv_args'])\n",
    "            for _ in range(meta['n_dense_inv'])\n",
    "        ])\n",
    "\n",
    "        self.weights_layer = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(**meta['dense_inv_args'])\n",
    "            for _ in range(meta['n_dense_inv'])\n",
    "        ] + \n",
    "        [\n",
    "            tf.keras.layers.Dense(meta['dense_inv_args']['units'])\n",
    "        \n",
    "        ])\n",
    "\n",
    "        self.post_pooling_dense = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(**meta['dense_inv_args'])\n",
    "            for _ in range(meta['n_dense_inv'])\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Transofrms the input into an invariant representation.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        x : tf.Tensor of shape (batch_size, n, m) - the input where n is the 'time' or 'samples' dimensions\n",
    "            over which pooling is performed and m is the input dimensionality\n",
    "        ----------\n",
    "\n",
    "        Returns:\n",
    "        out : tf.Tensor of shape (batch_size, h_dim) -- the pooled and invariant representation of the input\n",
    "        \"\"\"\n",
    "\n",
    "        # Embed\n",
    "        x = self.module(x)\n",
    "        \n",
    "        # Compute weights\n",
    "        w = tf.nn.softmax(self.weights_layer(x), axis=1)\n",
    "        w_x = tf.reduce_sum(x * w, axis=1)\n",
    "\n",
    "        # Increase representational power\n",
    "        out = self.post_pooling_dense(w_x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class EquivariantModule(tf.keras.Model):\n",
    "    \"\"\"Implements an equivariant nn module as proposed by Bloem-Reddy and Teh (2019).\"\"\"\n",
    "\n",
    "    def __init__(self, meta):\n",
    "        \"\"\"\n",
    "        Creates an equivariant neural network consisting of a FC network with\n",
    "        equal number of hidden units in each layer and an invariant module\n",
    "        with the same FC structure.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        meta : dict -- a dictionary with hyperparameter name - values\n",
    "        \"\"\"\n",
    "\n",
    "        super(EquivariantModule, self).__init__()\n",
    "\n",
    "        self.module = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(**meta['dense_equiv_args'])\n",
    "            for _ in range(meta['n_dense_equiv'])\n",
    "        ])\n",
    "\n",
    "        self.invariant_module = InvariantModule(meta)\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Transofrms the input into an equivariant representation.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        x : tf.Tensor of shape (batch_size, n, m) - the input where n is the 'time' or 'samples' dimensions\n",
    "            over which pooling is performed and m is the input dimensionality\n",
    "        ----------\n",
    "\n",
    "        Returns:\n",
    "        out : tf.Tensor of shape (batch_size, h_dim) -- the pooled and invariant representation of the input\n",
    "        \"\"\"\n",
    "\n",
    "        x_inv = self.invariant_module(x)\n",
    "        x_inv = tf.stack([x_inv] * int(x.shape[1]), axis=1) # Repeat x_inv n times\n",
    "        x = tf.concat((x_inv, x), axis=-1)\n",
    "        out = self.module(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class InvariantNetwork(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Implements a network which parameterizes a\n",
    "    permutationally invariant function according to Bloem-Reddy and Teh (2019).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, meta):\n",
    "        \"\"\"\n",
    "        Creates a permutationally invariant network\n",
    "        consisting of two equivariant modules and one invariant module.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        meta : dict -- hyperparameter settings for the equivariant and invariant modules\n",
    "        \"\"\"\n",
    "\n",
    "        super(InvariantNetwork, self).__init__()\n",
    "\n",
    "        self.equiv = tf.keras.Sequential([\n",
    "            EquivariantModule(meta)\n",
    "            for _ in range(meta['n_equiv'])\n",
    "        ])\n",
    "        self.inv = InvariantModule(meta)\n",
    "\n",
    "\n",
    "    def call(self, x, **kwargs):\n",
    "        \"\"\"\n",
    "        Transofrms the input into a permutationally invariant\n",
    "        representation by first passing it through multiple equivariant\n",
    "        modules in order to increase representational power.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        x : tf.Tensor of shape (batch_size, n, m) - the input where n is the 'time' or\n",
    "        'samples' dimensions over which pooling is performed and m is the input dimensionality\n",
    "        ----------\n",
    "\n",
    "        Returns:\n",
    "        out : tf.Tensor of shape (batch_size, h_dim) -- the pooled and invariant representation of the input\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.equiv(x)\n",
    "        out = self.inv(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = tf.random_normal(shape=(10, 50, 1))\n",
    "theta_g = tf.random_normal((1, 4))\n",
    "theta_p = tf.random_normal((1, 10, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalBayesFlow(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, meta):\n",
    "        super(HierarchicalBayesFlow, self).__init__()\n",
    "        \n",
    "        self.global_summary = InvariantNetwork(meta['summary_meta'])\n",
    "        self.summary_net = InvariantNetwork(meta['summary_meta'])\n",
    "        \n",
    "        # Invertible level 1\n",
    "        self.cINNs = [ConditionalInvertibleBlock(meta['inv']) \n",
    "                      for _ in range(meta['inv']['n_blocks'])]\n",
    "        \n",
    "        # Invertible level 2\n",
    "        self.cINNs_global = [ConditionalInvertibleBlock(meta['inv_global']) \n",
    "                            for _ in range(meta['inv_global']['n_blocks'])]\n",
    "        \n",
    "    def call(self, x, theta_g, theta_p):\n",
    "        \"\"\"\n",
    "        x is 3D Np x Nd x Nm\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute summaries\n",
    "        x_local = tf.expand_dims(self.summary_net(x), axis=0) # Np x Ns\n",
    "        x_global = self.global_summary(x_local)\n",
    "        x_global_r = tf.stack([x_global] * int(x_local.shape[1]), axis=1)\n",
    "        x_p = tf.concat([x_local, x_global_r], axis=-1)\n",
    "        \n",
    "        z_J_p = self.forward_inv(theta_p, x_p)\n",
    "        z_J_g = self.forward_inv_global(theta_g, x_global)\n",
    "         \n",
    "        return z_J_p, z_J_g\n",
    "    \n",
    "    def forward_inv(self, theta, x):\n",
    "        \"\"\"Performs a forward pass though the chain.\"\"\"\n",
    "\n",
    "        z = theta\n",
    "        log_det_Js = []\n",
    "        for cINN in self.cINNs:\n",
    "            z, log_det_J = cINN(z, x)\n",
    "            log_det_Js.append(log_det_J)\n",
    "        # Sum Jacobian determinants for all blocks to obtain total Jacobian.\n",
    "        log_det_J = tf.add_n(log_det_Js)\n",
    "        return {'z': z, 'log_det_J': log_det_J}\n",
    "    \n",
    "    def forward_inv_global(self, theta, x):\n",
    "        \n",
    "        z = theta\n",
    "        log_det_Js = []\n",
    "        for cINN in self.cINNs_global:\n",
    "            z, log_det_J = cINN(z, x)\n",
    "            log_det_Js.append(log_det_J)\n",
    "        # Sum Jacobian determinants for all blocks to obtain total Jacobian.\n",
    "        log_det_J = tf.add_n(log_det_Js)\n",
    "        return {'z': z, 'log_det_J': log_det_J}\n",
    "    \n",
    "    def inference(self, x):\n",
    "        \"\"\"Performs a reverse pass through the chain.\"\"\"\n",
    "\n",
    "        # Compute summaries\n",
    "        x_local = tf.expand_dims(self.summary_net(x), axis=0) # Np x Ns\n",
    "        x_global = self.global_summary(x_local)\n",
    "        x_global_r = tf.stack([x_global] * int(x_local.shape[1]), axis=1)\n",
    "        x_p = tf.concat([x_local, x_global_r], axis=-1)\n",
    "\n",
    "        # Infer hyperparams\n",
    "        z = tf.random_normal(shape=(1, 4))\n",
    "\n",
    "        theta_g = z\n",
    "        for cINN in reversed(self.cINNs_global):\n",
    "            theta_g = cINN(theta_g, x_global, inverse=True)\n",
    "        \n",
    "        # Infer local\n",
    "        z = tf.random_normal(shape=(1, 10, 2))\n",
    "        theta_p = z\n",
    "        for cINN in reversed(self.cINNs):\n",
    "            theta_p = cINN(theta_p, x_p, inverse=True)\n",
    "        return theta_g, theta_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum_likelihood_loss(z, log_det_J, **args):\n",
    "    \"\"\"\n",
    "    Computes the ML loss as described by Ardizzone et al. (in press).\n",
    "    ----------\n",
    "    Arguments:\n",
    "    z         : tf.Tensor of shape (batch_size, z_dim) -- the output of the final CC block f(x; c, W)\n",
    "    log_det_J : tf.Tensor of shape (batch_size, )      -- the log determinant of the jacobian computed the CC block.\n",
    "\n",
    "    Output:\n",
    "    loss : tf.Tensor of shape (,)  -- a single scalar Monte-Carlo approximation of E[ ||z||^2 / 2 - log|det(J)| ]\n",
    "    \"\"\"\n",
    "\n",
    "    return tf.reduce_mean(0.5 * tf.square(tf.norm(z, axis=-1)) - log_det_J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = {\n",
    "    'summary_meta': {\n",
    "        'dense_inv_args'   :  dict(units=32, activation='elu', kernel_initializer='glorot_normal'),\n",
    "        'dense_equiv_args' :  dict(units=16, activation='elu', kernel_initializer='glorot_normal'),\n",
    "        'n_dense_inv'      :  2,\n",
    "        'n_dense_equiv'    :  2,\n",
    "        'n_equiv'          :  2\n",
    "    },\n",
    "    'inv': {\n",
    "        'activation': 'elu',\n",
    "        'initializer': 'glorot_uniform',\n",
    "        'alpha': 1.9,\n",
    "        'theta_dim': 2,\n",
    "        'permute': None,\n",
    "        'n_blocks': 3,\n",
    "        'n_units': [64, 64]\n",
    "    },\n",
    "    'inv_global': {\n",
    "        'activation': 'elu',\n",
    "        'initializer': 'glorot_uniform',\n",
    "        'alpha': 1.9,\n",
    "        'theta_dim': 4,\n",
    "        'permute': None,\n",
    "        'n_blocks': 3,\n",
    "        'n_units': [64, 64]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n_p, n_d):\n",
    "    \n",
    "    mu_g = np.random.normal()\n",
    "    sigma_g = np.random.gamma(1)\n",
    "    a = np.random.exponential(1)\n",
    "    b = np.random.exponential(1)\n",
    "    \n",
    "    mu = np.random.normal(mu_g, sigma_g, size=n_p)\n",
    "    sigma = np.random.gamma(a, b, size=n_p)\n",
    "    \n",
    "    x = np.random.normal(mu, sigma, size=(n_d, n_p)).T[:, :, np.newaxis]\n",
    "    return (tf.convert_to_tensor(x, dtype=tf.float32), \n",
    "            tf.convert_to_tensor(np.array([[mu_g, sigma_g, a, b]]), dtype=tf.float32),\n",
    "            tf.convert_to_tensor(np.array([mu, sigma]).T[np.newaxis],dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = partial(generate_data, n_p=10, n_d=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_online(model, optimizer, data_gen, iterations, p_bar=None,\n",
    "                 clip_value=5., global_step=None, n_smooth=100):\n",
    "    \"\"\"\n",
    "    Performs a number of training iterations with a given tensorflow model and optimizer.\n",
    "\n",
    "    ----------\n",
    "\n",
    "    Arguments:\n",
    "    model           : tf.keras.Model -- a neural network model implementing a __call__() method\n",
    "    optimizer       : tf.train.Optimizer -- the optimizer used for backprop\n",
    "    data_gen        : callable -- a function providing batches of data\n",
    "    loss_fun        : callable -- a function computing the loss given model outputs\n",
    "    iterations      : int -- the number of training loops to perform\n",
    "    batch_size      : int -- the batch_size used for training\n",
    "    ----------\n",
    "\n",
    "    Keyword Arguments:\n",
    "    p_bar           : ProgressBar or None -- an instance for tracking the training progress\n",
    "    clip_value      : float       -- the value used for clipping the gradients\n",
    "    clip_method     : str         -- the method used for clipping (default 'global_norm')\n",
    "    global_step     : tf.Variavle -- a scalar tensor tracking the number of steps and used for learning rate decay  \n",
    "    ----------\n",
    "\n",
    "    Returns:\n",
    "    losses : a dictionary with regularization and loss evaluations at each training iteration\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare a dict for storing losses\n",
    "    losses = {\n",
    "        'loss': [],\n",
    "    }\n",
    "\n",
    "    # Run training loop\n",
    "    for it in range(1, iterations+1):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Generate inputs for the network\n",
    "            x, theta_g, theta_p = data_gen()\n",
    "\n",
    "\n",
    "            # Forward pass \n",
    "            z_J_p, z_J_g = model(x, theta_g, theta_p)\n",
    "            ml_p = maximum_likelihood_loss(z_J_p['z'], z_J_p['log_det_J'])\n",
    "            ml_g = maximum_likelihood_loss(z_J_g['z'], z_J_g['log_det_J']) \n",
    "            \n",
    "            # Loss computation and backward pass\n",
    "            total_loss = ml_p + ml_g\n",
    "\n",
    "        # One step backprop\n",
    "        gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "        if clip_value is not None:\n",
    "            gradients, _ = tf.clip_by_global_norm(gradients, clip_value)\n",
    "        optimizer.apply_gradients(zip(gradients, model.variables), global_step=global_step)\n",
    "\n",
    "        # Store losses\n",
    "        losses['loss'].append(total_loss)\n",
    "        running_loss = total_loss if it < n_smooth else np.mean(losses['loss'][-n_smooth:])\n",
    "\n",
    "        # Update progress bar\n",
    "        if p_bar is not None:\n",
    "            p_bar.set_postfix_str(\"Iteration: {0},Loss: {1:.3f},Running Loss: {2:.3f}\"\n",
    "            .format(it, total_loss, running_loss))\n",
    "            p_bar.update(1)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "starter_learning_rate = 0.001\n",
    "epochs = 50\n",
    "global_step = tfe.Variable(0, dtype=tf.int32)\n",
    "decay_steps = 1000\n",
    "iterations_per_epoch = 1000\n",
    "decay_rate = .95\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, decay_steps, decay_rate)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "model = HierarchicalBayesFlow(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%time\n",
    "# for ep in range(1, epochs+1):\n",
    "#     with tqdm(total=iterations_per_epoch, desc='Training epoch {}'.format(ep)) as p_bar:\n",
    "#         losses = train_online(model=model, \n",
    "#                               optimizer=optimizer, \n",
    "#                               data_gen=data_gen, \n",
    "#                               iterations=1000,\n",
    "#                               p_bar=p_bar,\n",
    "#                               global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored from ./checkpoints/hierarchical\\ckpt-1\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.Checkpoint(step=global_step, optimizer=optimizer, net=model)\n",
    "manager = tf.train.CheckpointManager(checkpoint, './checkpoints/{}'.format('hierarchical'), max_to_keep=3)\n",
    "checkpoint.restore(manager.latest_checkpoint)\n",
    "if manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, theta_g, theta_p = data_gen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_samples = []\n",
    "p_samples = []\n",
    "for _ in range(50):\n",
    "    s = model.inference(test)\n",
    "    g_samples.append(s[0].numpy())\n",
    "    p_samples.append(s[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3671413 ,  2.7803597 ,  1.2153327 ,  0.53016835]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(g_samples).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=8927806, shape=(1, 4), dtype=float32, numpy=\n",
       "array([[-0.67196506,  2.9674332 ,  1.4488682 ,  0.35598543]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.7528837 ,  1.3863918 ],\n",
       "       [-0.8163389 ,  0.02282362],\n",
       "       [ 4.1098576 ,  0.26752824],\n",
       "       [ 3.0472753 ,  0.3132162 ],\n",
       "       [ 0.5897017 ,  0.07296287],\n",
       "       [-1.8944311 ,  0.2140283 ],\n",
       "       [-1.7764124 ,  0.9352921 ],\n",
       "       [ 3.6297038 ,  0.03652804],\n",
       "       [-1.5804875 ,  0.5832627 ],\n",
       "       [ 0.29272157,  0.89911896]], dtype=float32)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(p_samples).mean(axis=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=8927807, shape=(1, 10, 2), dtype=float32, numpy=\n",
       "array([[[-3.236438  ,  0.9064149 ],\n",
       "        [-0.9176344 ,  0.01489944],\n",
       "        [ 4.8029294 ,  0.34953535],\n",
       "        [ 3.7479672 ,  0.37762964],\n",
       "        [ 0.6207832 ,  0.07409462],\n",
       "        [-2.1444337 ,  0.23153763],\n",
       "        [-2.0648587 ,  0.7919902 ],\n",
       "        [ 4.1491613 ,  0.02888046],\n",
       "        [-1.8619667 ,  0.44587857],\n",
       "        [ 0.1768958 ,  0.79432696]]], dtype=float32)>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
