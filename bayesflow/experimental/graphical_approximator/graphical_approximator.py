from collections.abc import Mapping, Sequence

import keras
import numpy as np

# TODO: add log_prob method to approximator
from ...adapters import Adapter
from ...approximators import Approximator
from ...networks import InferenceNetwork, SummaryNetwork
from ...networks.standardization import Standardization
from ...types import Shape
from ..graphical_simulator import SimulationOutput
from ..graphs import InvertedGraph
from .utils import (
    inference_condition_shapes_by_network,
    inference_conditions_by_network,
    inference_variable_shapes_by_network,
    inference_variables_by_network,
    prepare_inference_conditions,
    split_network_output,
    summary_input_shapes_by_network,
    summary_inputs_by_network,
    summary_outputs_by_network,
)


class GraphicalApproximator(Approximator):
    """
    Amortized inference for probabilistic models defined by directed acyclic graphs.

    This approximator implements amortized Bayesian inference for simulation-based
    models whose probabilistic structure can be expressed as a directed acyclic
    graph (DAG). The dependency structure is encoded in an
    :class:`bayesflow.experimental.graphs.InvertedGraph`, which specifies the order
    in which variables are summarized, conditioned on, and inferred.

    Parameters
    ----------
    graph : bayesflow.experimental.graphs.InvertedGraph
    adapter : bayesflow.adapters.Adapter
        Adapter for data processing. You can use :py:meth:`build_adapter`
        to create it.
    inference_networks : list[InferenceNetwork]
        The inference networks used for posterior estimation.
        Each network is responsible for a subset of variables, as
        determined by the network composition defined by the supplied `graph`.
        Call the :py:meth:`network_composition` method of the `InvertedGraph` for an
        overview of how variables are assigned to inference networks.lihood approximation.
    summary_networks : list[SummaryNetwork], optional
        The summary networks used for data summarization (default is None).
    standardize : str | Sequence[str] | None
        The variables to standardize before passing to the networks. Can be either
        "all" or any subset of the variables generated by the SimulationGraph contained
        in the `simulation_graph` field of the supplied InvertedGraph (default is "all").
    **kwargs : dict, optional
        Additional arguments passed to the :py:class:`bayesflow.approximators.Approximator` class.
    """

    def __init__(
        self,
        graph: InvertedGraph,
        *,
        adapter: Adapter,
        inference_networks: Sequence[InferenceNetwork],
        summary_networks: Sequence[SummaryNetwork] | None = None,
        standardize: str | Sequence[str] | None = "all",
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.graph = graph
        self.adapter = adapter
        self.inference_networks = inference_networks
        self.summary_networks = summary_networks
        self.data_shapes = None

        if isinstance(standardize, str) and standardize != "all":
            self.standardize = [standardize]
        else:
            self.standardize = standardize or []

        if standardize == "all":
            self.standardize_layers = None
        else:
            self.standardize_layers = {var: Standardization(trainable=False) for var in self.standardize}

    def build(self, data_shapes: dict[str, Shape]) -> None:
        # build summary networks
        input_shapes = summary_input_shapes_by_network(self, data_shapes)
        for i, summary_network in enumerate(self.summary_networks or []):
            if not summary_network.built:
                summary_network.build(input_shapes[i])

        # build inference networks
        variable_shapes = inference_variable_shapes_by_network(self, data_shapes)
        condition_shapes = inference_condition_shapes_by_network(self, data_shapes)

        for i, inference_network in enumerate(self.inference_networks or []):
            if not inference_network.built:
                inference_network.build(variable_shapes[i], condition_shapes[i])

        # build standardization layers
        if self.standardize == "all":
            # Only include variables present in data_shapes
            self.standardize = list(data_shapes.keys())
            self.standardize_layers = {var: Standardization(trainable=False) for var in self.standardize}

        for var in self.standardize:
            self.standardize_layers[var].build(data_shapes[var])

        self.data_shapes = data_shapes
        self.built = True

    def compute_metrics(self, stage: str = "training", **kwargs):
        """
        Computes loss and tracks metrics for the inference and summary networks.

        This method orchestrates the end-to-end computation of metrics and loss for a model
        with both inference and optional summary network. It handles standardization of input
        variables, combines summary outputs with inference conditions when necessary, and
        aggregates loss and all tracked metrics into a unified dictionary. The returned dictionary
        includes both the total loss and all individual metrics, with keys indicating their source.

        Parameters
        ----------
        stage : str, optional
            Current training stage (e.g., "training", "validation", "inference"). Controls
            the behavior of standardization and some metric computations (default is "training").

        Returns
        -------
        metrics : dict[str, Tensor]
            Dictionary containing the total loss under the key "loss", as well as all tracked
            metrics for the inference and summary networks. Each metric key is prefixed with
            "inference_" or "summary_" to indicate its source.
        """
        # compute summary metrics
        summary_inputs = summary_inputs_by_network(self, kwargs)
        summary_metrics = {}

        for i, summary_network in enumerate(self.summary_networks or []):
            summary_metrics[i] = summary_network.compute_metrics(summary_inputs[i], stage=stage)
            summary_metrics[i].pop("outputs")

        # compute inference metrics
        inference_conditions = inference_conditions_by_network(self, kwargs)
        inference_variables = inference_variables_by_network(self, kwargs)

        inference_metrics = {}
        for i, inference_network in enumerate(self.inference_networks):
            inference_metrics[i] = inference_network.compute_metrics(
                inference_variables[i], conditions=inference_conditions[i], stage=stage
            )

        # combine metrics
        total_loss = 0
        combined_metrics = {}

        for i, metric_type in enumerate([summary_metrics, inference_metrics]):
            prefix = "summary_metrics" if i == 0 else "infrence_metrics"
            for val, metrics in metric_type.items():
                if "loss" in metrics:
                    total_loss += metrics["loss"]
                for k, v in metrics.items():
                    combined_metrics[f"{prefix}_{val}/{k}"] = v

        return total_loss, combined_metrics

    def fit(self, *args, **kwargs):
        """
        Trains the approximator on the provided dataset or on-demand data generated from the given simulator.
        If `dataset` is not provided, a dataset is built from the `simulator`.
        If the model has not been built, it will be built using a batch from the dataset.

        Parameters
        ----------
        dataset : keras.utils.PyDataset, optional
            A dataset containing simulations for training. If provided, `simulator` must be None.
        simulator : Simulator, optional
            A simulator used to generate a dataset. If provided, `dataset` must be None.
        **kwargs
            Additional keyword arguments passed to `keras.Model.fit()`, including (see also `build_dataset`):

            batch_size : int or None, default='auto'
                Number of samples per gradient update. Do not specify if `dataset` is provided as a
                `keras.utils.PyDataset`, `tf.data.Dataset`, `torch.utils.data.DataLoader`, or a generator function.
            epochs : int, default=1
                Number of epochs to train the model.
            verbose : {"auto", 0, 1, 2}, default="auto"
                Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.
            callbacks : list of keras.callbacks.Callback, optional
                List of callbacks to apply during training.
            validation_split : float, optional
                Fraction of training data to use for validation (only supported if `dataset` consists of NumPy arrays
                or tensors).
            validation_data : tuple or dataset, optional
                Data for validation, overriding `validation_split`.
            shuffle : bool, default=True
                Whether to shuffle the training data before each epoch (ignored for dataset generators).
            initial_epoch : int, default=0
                Epoch at which to start training (useful for resuming training).
            steps_per_epoch : int or None, optional
                Number of steps (batches) before declaring an epoch finished.
            validation_steps : int or None, optional
                Number of validation steps per validation epoch.
            validation_batch_size : int or None, optional
                Number of samples per validation batch (defaults to `batch_size`).
            validation_freq : int, default=1
                Specifies how many training epochs to run before performing validation.

        Returns
        -------
        keras.callbacks.History
            A history object containing the training loss and metrics values.

        Raises
        ------
        ValueError
            If both `dataset` and `simulator` are provided or neither is provided.
        """

        if "dataset" in kwargs.keys():
            if isinstance(kwargs["dataset"], SimulationOutput):
                kwargs["dataset"] = kwargs["dataset"].data

        return super(GraphicalApproximator, self).fit(*args, **kwargs, adapter=self.adapter)

    def sample(self, *, num_samples: int, conditions: Mapping[str, np.ndarray]) -> Mapping[str, np.ndarray]:
        """
        Generates samples from the approximator given input conditions. The `conditions` dictionary is preprocessed
        using the `adapter`. Samples are converted to NumPy arrays after inference.

        Parameters
        ----------
        num_samples : int
            Number of samples to generate.
        conditions : dict[str, np.ndarray]
            Dictionary of conditioning variables as NumPy arrays.
        split : bool, default=False
            Whether to split the output arrays along the last axis and return one column vector per target variable
            samples.
        **kwargs : dict
            Additional keyword arguments for the adapter and sampling process.

        Returns
        -------
        dict[str, np.ndarray]
            Dictionary containing generated samples with the same keys as `conditions`.
        """
        summary_outputs = summary_outputs_by_network(self, conditions)
        batch_size = keras.ops.shape(summary_outputs[0])[0]
        data_node = self.graph.simulation_graph.data_node()
        variable_names = self.graph.simulation_graph.variable_names()

        inference_conditions = {}

        # add num_samples as repeats across the batch dimension
        for name in variable_names[data_node]:
            inference_conditions[name] = keras.ops.repeat(conditions[name], num_samples, axis=0)

        for i, inference_network in enumerate(self.inference_networks):
            cond = prepare_inference_conditions(self, inference_conditions, i)
            samples = inference_network.sample((batch_size * num_samples,), conditions=cond)
            split_output = split_network_output(self, samples, i)

            for k, v in split_output.items():
                inference_conditions[k] = v

        # build sample dict with introduced num_samples dimension at axis 1
        sample_dict = {}
        for k, v in inference_conditions.items():
            if k not in variable_names[data_node]:
                target_shape = (batch_size, num_samples, *keras.ops.shape(v)[1:])
                sample_dict[k] = keras.ops.convert_to_numpy(keras.ops.reshape(v, target_shape))

        return sample_dict

    def _batch_size_from_data(self, data):
        """
        Fetches the current batch size from an input dictionary.
        """
        data_shapes = self.data_shapes(data)
        batch_size = next(iter(data_shapes.values()))[0]

        return batch_size

    def _data_shapes(self, adapted_data: SimulationOutput | Mapping) -> Mapping:
        if isinstance(adapted_data, dict):
            return keras.tree.map_structure(keras.ops.shape, adapted_data)
        elif isinstance(adapted_data, SimulationOutput):
            return keras.tree.map_structure(keras.ops.shape, adapted_data.data)

        return {}
