{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy.stats import levy_stable\n",
    "\n",
    "from numba import njit\n",
    "from numba.typed import List\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesflow.networks import InvertibleNetwork, InvariantNetwork\n",
    "from bayesflow.amortizers import SingleModelAmortizer\n",
    "from bayesflow.trainers import ParameterEstimationTrainer\n",
    "from bayesflow.diagnostics import *\n",
    "from bayesflow.models import GenerativeModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulator settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "code_folding": [
     35,
     56,
     71,
     91,
     100,
     120
    ]
   },
   "outputs": [],
   "source": [
    "def prior(batch_size):\n",
    "    \"\"\"\n",
    "    Samples from the prior 'batch_size' times.\n",
    "    ----------\n",
    "    \n",
    "    Arguments:\n",
    "    batch_size : int -- the number of samples to draw from the prior\n",
    "    ----------\n",
    "    \n",
    "    Output:\n",
    "    theta : np.ndarray of shape (batch_size, theta_dim) -- the samples batch of parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prior ranges for the simulator \n",
    "    # v_c ~ U(-7.0, 7.0)\n",
    "    # a_c ~ U(0.1, 4.0)\n",
    "    # t0 ~ U(0.1, 3.0)\n",
    "    \n",
    "    a = np.random.gamma(2,2, size=batch_size)\n",
    "    zr = np.random.beta(5,5, size=batch_size)\n",
    "    v1 = np.random.normal(0,5, size=batch_size)\n",
    "    v2 = np.random.normal(0,5, size=batch_size)\n",
    "    v3 = np.random.normal(0,5, size=batch_size)\n",
    "    v4 = np.random.normal(0,5, size=batch_size)\n",
    "    t0 = np.random.gamma(2,2, size=batch_size)\n",
    "    alpha = 2*np.random.beta(2,1, size=batch_size)\n",
    "    \n",
    "    p_samples = np.c_[\n",
    "        a, zr, v1, v2, v3, v4, t0, alpha\n",
    "    ]\n",
    "    \n",
    "    return p_samples.astype(np.float32)\n",
    "\n",
    "\n",
    "@njit\n",
    "def diffusion_trial(v, a, ndt, zr, dt, max_steps):\n",
    "    \"\"\"Simulates a trial from the diffusion model.\"\"\"\n",
    "\n",
    "    n_steps = 0.\n",
    "    x = a * zr\n",
    "\n",
    "    # Simulate a single DM path\n",
    "    while (x > 0 and x < a and n_steps < max_steps):\n",
    "\n",
    "        # DDM equation\n",
    "        x += v*dt + np.sqrt(dt) * np.random.normal()\n",
    "\n",
    "        # Increment step\n",
    "        n_steps += 1.0\n",
    "\n",
    "    rt = n_steps * dt\n",
    "    return rt + ndt if x > 0. else -rt - ndt\n",
    "\n",
    "\n",
    "\n",
    "@njit\n",
    "def diffusion_2_conds(params, n_trials, dt=0.005, max_steps=1e4):\n",
    "    \"\"\"\n",
    "    Simulates a diffusion process for 2 conditions with 5 parameters (v1, v2, a1, a2, ndt).\n",
    "    \"\"\"\n",
    "    \n",
    "    n_trials_c1 = n_trials[0]\n",
    "    n_trials_c2 = n_trials[1]\n",
    "    \n",
    "    v1, v2, a1, a2, ndt = params\n",
    "    rt_c1 = diffusion_condition(n_trials_c1, v1, a1, ndt,  dt=dt, max_steps=max_steps)\n",
    "    rt_c2 = diffusion_condition(n_trials_c2, v2, a2, ndt, dt=dt, max_steps=max_steps)\n",
    "    rts = np.concatenate((rt_c1, rt_c2))\n",
    "    return rts\n",
    "\n",
    "@njit\n",
    "def levy_trial(noise, a=1, zr=0.5, v=0, ndt=1, alpha=2, dt=0.001, max_steps=1e4):\n",
    "    \"\"\"Simulates a trial from the levy model.\"\"\"\n",
    "\n",
    "    n_steps = 0\n",
    "    x = a * zr\n",
    "\n",
    "    # Simulate a single DM path\n",
    "    while (x > 0 and x < a and n_steps < max_steps):\n",
    "\n",
    "        # model equation\n",
    "        x += v*dt + (dt ** (1/alpha))  * noise[n_steps]\n",
    "\n",
    "        # Increment step\n",
    "        n_steps += 1\n",
    "\n",
    "    rt = n_steps * dt\n",
    "    return rt + ndt if x > 0. else -rt - ndt\n",
    "\n",
    "\n",
    "@njit\n",
    "def levy_condition(n_trials, noise, a=1, zr=0.5, v=0, ndt=1, alpha=2, dt=0.001, max_steps=1e4):\n",
    "    \"\"\"Simulates a Levy process over an entire condition.\"\"\"\n",
    "    \n",
    "    x = np.empty(n_trials)\n",
    "    for n in range(n_trials):\n",
    "        x[n] = levy_trial(noise[n], a, zr, v, ndt, alpha, dt, max_steps)\n",
    "    return x\n",
    "\n",
    "@njit\n",
    "def levy_4_conds(params, n_trials, noise, dt=0.001, max_steps=1e4):\n",
    "    \"\"\"\n",
    "    Simulates a levy process for 4 conditions with 8 parameters (a, zr, v1, v2, v3, v4, t0, alpha).\n",
    "    \"\"\"\n",
    "    \n",
    "    n_trials_c1 = n_trials[0]\n",
    "    n_trials_c2 = n_trials[1]\n",
    "    n_trials_c3 = n_trials[2]\n",
    "    n_trials_c4 = n_trials[3]\n",
    "    \n",
    "    a, zr, v1, v2, v3, v4, ndt, alpha = params\n",
    "    rt_c1 = levy_condition(n_trials[0], noise[0], a, zr, v1, ndt, alpha, dt=dt, max_steps=max_steps)\n",
    "    rt_c2 = levy_condition(n_trials[1], noise[1], a, zr, v2, ndt, alpha, dt=dt, max_steps=max_steps)\n",
    "    rt_c3 = levy_condition(n_trials[2], noise[2], a, zr, v3, ndt, alpha, dt=dt, max_steps=max_steps)\n",
    "    rt_c4 = levy_condition(n_trials[3], noise[3], a, zr, v4, ndt, alpha, dt=dt, max_steps=max_steps)\n",
    "\n",
    "    rts = np.concatenate((rt_c1, rt_c2, rt_c3, rt_c4))\n",
    "    return rts\n",
    "\n",
    "\n",
    "def batch_simulator(prior_samples, n_obs, dt=0.001, s=1.0, max_steps=1e4):\n",
    "    \"\"\"\n",
    "    Simulate multiple diffusion_model_datasets.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_sim = prior_samples.shape[0]\n",
    "    sim_data = np.zeros((n_sim, n_obs), dtype=np.float32)\n",
    "    \n",
    "    n1 = n2 = n3 = n_obs // 4\n",
    "    n4 = n_obs - 3 * n1\n",
    "    n_obs_tuple = (n1, n2, n3, n4)\n",
    "    \n",
    "    # Simulate diffusion data\n",
    "    for i in range(n_sim):\n",
    "        \n",
    "        # Precompute noise, shape of each array in the list will be (n_obs_cond, max_steps)\n",
    "        noise = List([\n",
    "            levy_stable.rvs(alpha=prior_samples[i,-1], beta=0, size=(n, int(max_steps)))\n",
    "            for n in n_obs_tuple \n",
    "        ])\n",
    "        \n",
    "        # Simulate data\n",
    "        sim_data[i] = levy_4_conds(prior_samples[i], n_obs_tuple, noise)\n",
    "        \n",
    "    # Create condition labels\n",
    "    cond_arr = np.stack(n_sim * [np.concatenate((np.zeros(n1), np.ones(n2), 2*np.ones(n3), 3*np.ones(n3)))] )\n",
    "    sim_data = np.stack((sim_data, cond_arr), axis=-1)\n",
    "    \n",
    "    return sim_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# params = prior(10)\n",
    "# batch_simulator(params, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An example Bayesian workflow (with BayesFlow)\n",
    "\n",
    "Towards a principled Bayesian workflow for cognitive modeling:\n",
    "\n",
    "https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html\n",
    "\n",
    "https://arxiv.org/abs/1904.12765"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior predictive checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an amortized parameter estimation network\n",
    "Here, we use an invariant summary network and an invertible inference network with default settings.\n",
    "\n",
    "We connect the networks through a *SingleModelAmortizer* instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_net = InvariantNetwork()\n",
    "inference_net = InvertibleNetwork({'n_params': 8})\n",
    "amortizer = SingleModelAmortizer(inference_net, summary_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We connect the prior and simulator through a *GenerativeModel* class which will take care of forward inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_model = GenerativeModel(prior, batch_simulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ParameterEstimationTrainer(\n",
    "    network=amortizer, \n",
    "    generative_model=generative_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using pre-simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-simulated data (could be loaded from somewhere else)\n",
    "n_sim = 5000\n",
    "n_obs = 100\n",
    "true_params, x = generative_model(n_sim, n_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "losses = trainer.train_offline(epochs=1, batch_size=64, params=true_params, sim_data=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using internally simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "losses = trainer.simulate_and_train_offline(n_sim=1000, epochs=2, batch_size=32, n_obs=n_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed n_obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "losses = trainer.train_online(epochs=2, iterations_per_epoch=100, batch_size=32, n_obs=n_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable n_obs\n",
    "def prior_N(n_min=60, n_max=300):\n",
    "    \"\"\"\n",
    "    A prior or the number of observation (will be called internally at each backprop step).\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.random.randint(n_min, n_max+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "losses = trainer.train_online(epochs=2, iterations_per_epoch=100, batch_size=32, n_obs=prior_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round-based training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "losses = trainer.train_rounds(epochs=1, rounds=5, sim_per_round=200, batch_size=32, n_obs=n_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience-replay training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "losses = trainer.train_experience_replay(epochs=3, \n",
    "                                         batch_size=32, \n",
    "                                         iterations_per_epoch=100, \n",
    "                                         capacity=100,\n",
    "                                         n_obs=prior_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_meta = {\n",
    "    'n_dense_s1': 2,\n",
    "    'n_dense_s2': 2,\n",
    "    'n_dense_s3': 2,\n",
    "    'n_equiv':    2,\n",
    "    'dense_s1_args': {'activation': 'relu', 'units': 64},\n",
    "    'dense_s2_args': {'activation': 'relu', 'units': 64},\n",
    "    'dense_s3_args': {'activation': 'relu', 'units': 64},\n",
    "}\n",
    "\n",
    "bf_meta = {\n",
    "    'n_coupling_layers': 4,\n",
    "    's_args': {\n",
    "        'units': [64, 64, 64],\n",
    "        'activation': 'elu',\n",
    "        'initializer': 'glorot_uniform',\n",
    "    },\n",
    "    't_args': {\n",
    "        'units': [64, 64, 64],\n",
    "        'activation': 'elu',\n",
    "        'initializer': 'glorot_uniform',\n",
    "    },\n",
    "    'n_params': 5,\n",
    "    'alpha': 1.9,\n",
    "    'permute': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_net = InvariantNetwork(sum_meta)\n",
    "inference_net = InvertibleNetwork(bf_meta)\n",
    "amortizer = SingleModelAmortizer(inference_net, summary_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compuational faithfulness\n",
    "(Via simulation-based calibration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sbc = 5000\n",
    "n_post_samples_sbc = 250\n",
    "params_sbc = prior(n_sbc)\n",
    "x_sbc = batch_simulator(params_sbc, 100)\n",
    "param_samples = np.concatenate([amortizer.sample(x, n_post_samples_sbc) \n",
    "                                for x in tf.split(x_sbc, 10, axis=0)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plot_sbc(param_samples, params_sbc, param_names=['v1', 'v2', 'a1', 'a2', 'ndt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model sensitivity/adequacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick and dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate (quick and dirty)\n",
    "true_params = prior(300)\n",
    "x = batch_simulator(true_params).astype(np.float32)\n",
    "param_samples = amortizer.sample(x, n_samples=1000)\n",
    "param_means = param_samples.mean(axis=0)\n",
    "true_vs_estimated(true_params, param_means, ['v1', 'v2', 'a1', 'a2','ndt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Bayesian eyechart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Simulate\n",
    "param_names = ['v1', 'v2', 'a1', 'a2','ndt']\n",
    "n_sim_s = 500\n",
    "n_samples_posterior = 1000\n",
    "true_params = prior(n_sim_s)\n",
    "x = batch_simulator(true_params)\n",
    "\n",
    "# Sample from posterior\n",
    "param_samples = amortizer.sample(x, n_samples_posterior)\n",
    "\n",
    "### Posterior z-score\n",
    "# Compute posterior means and stds\n",
    "post_means = param_samples.mean(0)\n",
    "post_stds = param_samples.std(0)\n",
    "post_vars = param_samples.var(0)\n",
    "\n",
    "# Compute posterior z score\n",
    "post_z_score = (post_means - true_params) / post_stds\n",
    "\n",
    "### Posterior contraction, i.e., 1 - post_var / prior_var\n",
    "prior_a = (0.1, 0.1, 0.1, 0.1, 0.1) # lower bound of uniform prior\n",
    "prior_b = (7.0, 7.0, 4.0, 4.0, 3.0) # upper bound of uniform prior\n",
    "\n",
    "# Compute prior vars analytically\n",
    "prior_vars = np.array([(b-a)**2/12 for a,b in zip(prior_a, prior_b)])\n",
    "post_cont = 1 - post_vars / prior_vars\n",
    "\n",
    "# Plotting time\n",
    "f, axarr = plt.subplots(2, 3, figsize=(12, 6))\n",
    "for i, (p, ax) in enumerate(zip(param_names, axarr.flat)):\n",
    "    \n",
    "\n",
    "    ax.scatter(post_cont[:, i], post_z_score[:, i], color='#8f2727', alpha=0.7)\n",
    "    ax.set_title(p, fontsize=20)\n",
    "    sns.despine(ax=ax)\n",
    "    ax.set_xlim([-0.1, 1.05])\n",
    "    ax.set_ylim([-3.5, 3.5])\n",
    "    ax.grid(color='black', alpha=0.1)\n",
    "    ax.set_xlabel('Posterior contraction', fontsize=14)\n",
    "    if i == 0 or i == 3:\n",
    "        ax.set_ylabel('Posterior z-score', fontsize=14)\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior postdictive/predictive checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
