{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Quickstart: Amortized Posterior Estimation",
   "id": "59db40e31138edf4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T21:31:41.493384Z",
     "start_time": "2024-11-08T21:31:30.624249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# ensure the backend is set\n",
    "import os\n",
    "if \"KERAS_BACKEND\" not in os.environ:\n",
    "    # set this to \"torch\", \"tensorflow\", or \"jax\"\n",
    "    os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "import keras\n",
    "\n",
    "# for BayesFlow devs: this ensures that the latest dev version can be found\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import bayesflow as bf"
   ],
   "id": "79203712506cdd75",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Welcome to the very first tutorial on using BayesFlow for amortized posterior estimation! In this notebook, we will estimate the means of a multivariate Gaussian model and illustrate some features of the library along the way.\n",
    "\n",
    "Here is a brief description of amortized posterior estimation:\n",
    "\n",
    "In traditional posterior estimation, as in Bayesian inference, we seek to compute or approximate the posterior distribution of model parameters given observed data for each new data instance separately. This process can be computationally expensive, especially for complex models or large datasets, because it often involves iterative optimization or sampling methods. This step needs to be repeated for each new instance of data.\n",
    "\n",
    "Amortized posterior estimation offers a solution to this problem. “Amortization” here refers to spreading out the computational cost over multiple instances. Instead of computing a new posterior from scratch for each data instance, amortized inference learns a function. This function is parameterized by a neural network, that directly maps observations to an approximation of the posterior distribution. This function is trained over the dataset to approximate the posterior for any new data instance efficiently. In this example, we will use a simple Gaussian model to illustrate the basic concepts of amortized posterior estimation."
   ],
   "id": "7104e1cc30b39a9d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Generative Model Definition\n",
    "\n",
    "From the perspective of the BayesFlow framework, a generative model is more than just a prior (encoding beliefs about the parameters before observing data) and a simulator (a likelihood function, often implicit, that generates data given parameters). In addition, it consists of various implicit context assumptions, which we can make explicit at any time. Furthermore, we can also amortize over these context variables, thus making our real-world inference more flexible (i.e., applicable to more contexts). We are leveraging the concept of amortized inference and extending it to context variables as well. The figure below illustrates the skeleton of a generative model as conceptualized in the BayesFlow framework.\n",
    "\n",
    "This conceptual model allows you to tackle very flexible model families with BayesFlow, as well as various other Bayesian tasks, such as prior sensitivity analysis or multiverse analysis.\n",
    "\n",
    "Prior sensitivity analysis: it is a technique used in Bayesian statistics to assess how sensitive the results of a model are to the choice of the prior distribution. In Bayesian inference, the prior represents our existing knowledge or assumptions about the parameters before observing the data. However, the selection of an appropriate prior can sometimes be subjective, and different priors can lead to different posterior estimates. Prior sensitivity analysis involves systematically varying the priors and examining how these variations affect the posterior estimates.\n",
    "\n",
    "The toy Gaussian model we will use for this tutorial takes a particularly simple form:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{\\mu} &\\sim \\mathcal{N}_D(\\mathbf{0}, \\sigma_0 \\mathbb{I}),\\\\\n",
    "    \\mathbf{x}_n &\\sim \\mathcal{N}_D(\\mathbf{\\mu}, \\sigma_1 \\mathbb{I}) \\quad \\text{for } n = 1, ..., N,\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathcal{N}_D$\n",
    "denotes a multivariate Gaussian (normal) density with $D$\n",
    "dimensions, which we set at $D = 4$\n",
    "for the current example. For simplicity, we will also set $\\sigma_0 = 1$\n",
    "and $\\sigma_1 = 1$\n",
    ". We will now implement this model using the latest numpy interface."
   ],
   "id": "9a0fa37dbcf904e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2327960a57c16ac6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
