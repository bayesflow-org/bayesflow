{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving from Bayesflow 1.0 to 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current users of bayesflow will notice that with the update to 2.0 many things have changed or been moved. This short guide aims to clarify what has been changed as well as additonal functionalities that have been added. This guide follows a similar structure to the Quickstart notebook, without the mathematical explaination in order to demonstrate the differences in workflow. However, for a more detailed explaination of any of the features, users should read any of the other example notebooks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Major Changes \n",
    "\n",
    "One of the major changes from _bayesflow 1.0_ to _bayeflow 2.0_ is that entire package has been reformatted in line with keras standards. This was done to allow users to choose their prefered backend for machine learning models. Rather than only being compatible with tensorflow, users can now choose to fit their models with either `TensorFlow`, `JAX` or `Pytorch`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ensure the backend is set\n",
    "import os\n",
    "if \"KERAS_BACKEND\" not in os.environ:\n",
    "    # set this to \"torch\", \"tensorflow\", or \"jax\"\n",
    "    os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import keras\n",
    "import bayesflow as bf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version of bayeflow also relies much more heavily on dictionaries. Nearly every object, and function will expect a dictionary, so any parameter or data should be returned as a dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Workflow \n",
    "\n",
    "### 1. Priors and Likelihood Model\n",
    "\n",
    "Previously users would define a prior function, which would then be used by a `Prior` object to sample prior values. The likelihood would then also be specified via function and used by a `Simulator` wrapper to produce observations for a given prior. These were then combined in the `GenerativeModel`, however this has been changed, we no longer use the `Prior`, `Simulator` or `GenerativeModel` objects. Instead `GenerativeModel` has been renamed to `simulator` which is a  single function that glues the prior and likelihood together.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theta_prior():\n",
    "    theta = np.random.normal(size=4)\n",
    "    # previously: \n",
    "    # return theta \n",
    "    return dict(theta=theta) # notice we return a dictionary\n",
    "    \n",
    "\n",
    "def likelihood_model(theta, n_obs):\n",
    "    x = np.random.normal(loc=theta, size=(n_obs, theta.shape[0]))\n",
    "    return dict(x=x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously the prior and likelihood were defined as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do Not Run\n",
    "prior = bf.simulation.Prior(prior_fun=theta_prior)\n",
    "simulator = bf.simulation.Simulator(simulator_fun=likelihood_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the new framework we also a define a meta function which allows us to dynamically set the batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta(batch_size):\n",
    "    return dict(n_obs=50)\n",
    "\n",
    "simulator = bf.make_simulator([theta_prior, likelihood_model], meta_fn=meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Adapter and Data Configuration\n",
    "\n",
    "In _bayesflow2.0_ we now need to specify the data configuration. For example we should specify which variables are `summary_variables` meaning observations that will be summarized in the summary network, the `inference_variables` meaning the prior draws on which we're interested in training the posterior network and the `inference_conditions` which specify our number of observations. Previously these things were inferred from the type of network used, but now they should be defined explictly with  the `adapter`. This allows users to ???  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter = (\n",
    "    bf.adapters.Adapter()\n",
    "    .to_array()\n",
    "    .broadcast(\"n_obs\")\n",
    "    .convert_dtype(from_dtype=\"float64\", to_dtype=\"float32\")\n",
    "    .standardize(exclude=[\"n_obs\"])\n",
    "    .rename(\"x\", \"summary_variables\")\n",
    "    .rename(\"theta\", \"inference_variables\")\n",
    "    .rename(\"n_obs\", \"inference_conditions\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition the adapter now has built in functions to transform data such as standardization or one-hot encoding. For a full list of the adapter transforms, please see the documentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Summary Network and Inference Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Approximator (Amortizer Posterior)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
