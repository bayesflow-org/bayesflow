{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.contrib.eager as tfe\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "from simulate_diffusion import simulate_diffusion\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Discriminate between the Ratcliff Diffusion Model and the Levy Flight Model (LFM)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     22,
     42,
     45,
     66
    ]
   },
   "outputs": [],
   "source": [
    "class InvariantModule(tf.keras.Model):\n",
    "    \"\"\"Implements an invariant nn module as proposed by Bloem-Reddy and Teh (2019).\"\"\"\n",
    "\n",
    "    def __init__(self, h_dim, n_dense=3):\n",
    "        \"\"\"\n",
    "        Creates an invariant function with mean pooling.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        h_dim   : int -- the number of hidden units in each of the modules\n",
    "        n_dense : int -- the number of dense layers of the modules\n",
    "        \"\"\"\n",
    "        \n",
    "        super(InvariantModule, self).__init__()\n",
    "        \n",
    "        self.module = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(h_dim, activation='elu', kernel_initializer='glorot_uniform') \n",
    "            for _ in range(n_dense)\n",
    "        ])\n",
    "\n",
    "        self.post_pooling_dense = tf.keras.layers.Dense(h_dim, activation='elu', kernel_initializer='glorot_uniform')   \n",
    "        \n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Transofrms the input into an invariant representation.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        x : tf.Tensor of shape (batch_size, n, m) - the input where n is the 'time' or 'samples' dimensions \n",
    "            over which pooling is performed and m is the input dimensionality\n",
    "        ----------\n",
    "\n",
    "        Returns:\n",
    "        out : tf.Tensor of shape (batch_size, h_dim) -- the pooled and invariant representation of the input\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.module(x)\n",
    "        x = tf.reduce_mean(x, axis=1)\n",
    "        out = self.post_pooling_dense(x)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class EquivariantModule(tf.keras.Model):\n",
    "    \"\"\"Implements an equivariant nn module as proposed by Bloem-Reddy and Teh (2019).\"\"\"\n",
    "\n",
    "    def __init__(self, h_dim, n_dense=3):\n",
    "        \"\"\"\n",
    "        Creates an equivariant neural network consisting of a FC network with\n",
    "        equal number of hidden units in each layer and an invariant module\n",
    "        with the same FC structure.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        h_dim   : int -- the number of hidden units in each of the modules\n",
    "        n_dense : int -- the number of dense layers of the modules\n",
    "        \"\"\"\n",
    "        \n",
    "        super(EquivariantModule, self).__init__()\n",
    "        \n",
    "        self.module = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(h_dim, activation='elu') \n",
    "            for _ in range(n_dense)\n",
    "        ])\n",
    "        \n",
    "        self.invariant_module = InvariantModule(h_dim, n_dense)\n",
    "        \n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Transofrms the input into an equivariant representation.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        x : tf.Tensor of shape (batch_size, n, m) - the input where n is the 'time' or 'samples' dimensions \n",
    "            over which pooling is performed and m is the input dimensionality\n",
    "        ----------\n",
    "\n",
    "        Returns:\n",
    "        out : tf.Tensor of shape (batch_size, h_dim) -- the pooled and invariant representation of the input\n",
    "        \"\"\"\n",
    "\n",
    "        x_inv = self.invariant_module(x)\n",
    "        x_inv = tf.stack([x_inv] * int(x.shape[1]), axis=1) # Repeat x_inv n times\n",
    "        x = tf.concat((x_inv, x), axis=-1)\n",
    "        out = self.module(x)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class InvariantNetwork(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Implements a network which parameterizes a \n",
    "    permutationally invariant function according to Bloem-Reddy and Teh (2019).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h_dim, n_dense=3, n_equiv=2):\n",
    "        \"\"\"\n",
    "        Creates a permutationally invariant network \n",
    "        consisting of two equivariant modules and one invariant module.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        h_dim   : int -- the number of hidden units in each of the modules\n",
    "        n_dense : int -- the number of dense layers of the modules\n",
    "        n_equiv : int -- the number of equivariant modules \n",
    "        \"\"\"\n",
    "        \n",
    "        super(InvariantNetwork, self).__init__()\n",
    "        \n",
    "        self.equiv = tf.keras.Sequential([\n",
    "            EquivariantModule(h_dim, n_dense)\n",
    "            for _ in range(n_equiv)\n",
    "        ])\n",
    "        self.inv = InvariantModule(h_dim, n_dense)\n",
    "        \n",
    "    def call(self, x, **kwargs):\n",
    "        \"\"\"\n",
    "        Transofrms the input into a permutationally invariant \n",
    "        representation by first passing it through multiple equivariant \n",
    "        modules in order to increase representational power.\n",
    "        ----------\n",
    "\n",
    "        Arguments:\n",
    "        x : tf.Tensor of shape (batch_size, n, m) - the input where n is the 'time' or \n",
    "        'samples' dimensions over which pooling is performed and m is the input dimensionality\n",
    "        ----------\n",
    "\n",
    "        Returns:\n",
    "        out : tf.Tensor of shape (batch_size, h_dim) -- the pooled and invariant representation of the input\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.equiv(x)\n",
    "        out = self.inv(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "# class ModelSelector(tf.keras.Model):\n",
    "    \n",
    "#     def __init__(self, k=2, inv_h_dim=32, n_equiv=2, z_dim=2, n_cond=2, n_dense=2, summary_dim=128):\n",
    "#         super(ModelSelector, self).__init__()\n",
    "        \n",
    "#         # Invariant net\n",
    "#         self.inv = [InvariantNetwork(inv_h_dim, n_equiv) for _ in range(n_cond)]\n",
    "        \n",
    "#         # Z mapping\n",
    "#         self.Wo = tf.keras.layers.Dense(summary_dim, use_bias=False)\n",
    "#         self.z_mapper = tf.keras.Sequential([\n",
    "#             tf.keras.layers.Dense(summary_dim, activation='elu', kernel_initializer='glorot_uniform') \n",
    "#             for _ in range(n_dense)\n",
    "#         ] +\n",
    "#         [tf.keras.layers.Dense(z_dim * 2, kernel_initializer='glorot_uniform')])\n",
    "        \n",
    "#         # Sigmoid classifier\n",
    "#         self.classifier = tf.keras.layers.Dense(1, kernel_initializer='glorot_uniform')\n",
    "#         self.n_cond = n_cond\n",
    "        \n",
    "#     def call(self, x, return_prob=False):\n",
    "#         \"\"\"Encodes x into z, samples and decodes.\"\"\"\n",
    "        \n",
    "#         # Encode\n",
    "#         x = tf.split(x, self.n_cond, axis=-1)\n",
    "#         x = tf.concat([net(x_cond) for x_cond, net in zip(x, self.inv)], axis=-1)\n",
    "#         x = self.Wo(x)\n",
    "        \n",
    "#         # Get z\n",
    "#         x = self.z_mapper(x)\n",
    "#         z_mean, z_logvar = tf.split(x, 2, axis=-1)\n",
    "        \n",
    "#         # Sample\n",
    "#         eps = tf.random_normal(shape=z_mean.shape)\n",
    "#         z = z_mean + eps * tf.exp(z_logvar * 0.5)\n",
    "        \n",
    "#         # Decode (returns model index)\n",
    "#         m_hat = self.classifier(z)\n",
    "#         if return_prob:\n",
    "#             m_hat = tf.sigmoid(m_hat)\n",
    "#         return z_mean, z_logvar, m_hat\n",
    "    \n",
    "#     def sample(self, x, n_samples):\n",
    "#         \"\"\"Samples model probabilities given data.\"\"\"\n",
    "        \n",
    "#         x = self.encoder(x)\n",
    "#         x = self.z_mapper(x)\n",
    "#         z_mean, z_logvar = tf.split(x, 2, axis=-1)\n",
    "#         eps = tf.random_normal(shape=(n_samples, z_mean.shape[0], z_mean.shape[1]))\n",
    "#         z = z_mean + eps * tf.exp(z_logvar * 0.5)\n",
    "#         z = tf.transpose(z, [1, 0, 2])\n",
    "#         m_probs = tf.sigmoid(self.classifier(z))\n",
    "#         return m_probs\n",
    "\n",
    "class ModelSelector(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, n_cond=2, inv_h_dim=128, n_equiv=3, z_dim=32, \n",
    "                 summary_dim=32, cls_dim=64, n_dense_cls=2):\n",
    "        super(ModelSelector, self).__init__()\n",
    "        \n",
    "        self.deterministic_encoder = [InvariantNetwork(h_dim=inv_h_dim, n_equiv=n_equiv) for _ in range(n_cond)]\n",
    "        self.probabilistic_encoder = [InvariantNetwork(h_dim=inv_h_dim, n_equiv=n_equiv) for _ in range(n_cond)]\n",
    "        self.Wd = tf.keras.layers.Dense(summary_dim, use_bias=False)\n",
    "        self.Wp = tf.keras.layers.Dense(summary_dim, use_bias=False)\n",
    "        self.z_mapper = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(z_dim * 2)\n",
    "        ])\n",
    "        \n",
    "        self.classifier = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(cls_dim, activation='elu')\n",
    "            for _ in range(n_dense_cls)\n",
    "        ] + [tf.keras.layers.Dense(1)])\n",
    "        \n",
    "        self.n_cond = n_cond\n",
    "        \n",
    "    def call(self, x, return_prob=False):\n",
    "        \n",
    "        # Encode deterministically\n",
    "        x_d = tf.split(x, self.n_cond, axis=-1)\n",
    "        x_d = tf.concat([net(x_cond) for x_cond, net in zip(x_d, self.deterministic_encoder)], axis=-1)\n",
    "        x_d = self.Wd(x_d)\n",
    "        \n",
    "        # Encode probabilistically\n",
    "        x_p = tf.split(x, self.n_cond, axis=-1)\n",
    "        x_p = tf.concat([net(x_cond) for x_cond, net in zip(x_p, self.probabilistic_encoder)], axis=-1)\n",
    "        x_p = self.Wp(x_p)\n",
    "        \n",
    "        # Sample z\n",
    "        x_p = self.z_mapper(x_p)\n",
    "        z_mean, z_logvar = tf.split(x_p, 2, axis=-1)\n",
    "        eps = tf.random_normal(shape=z_mean.shape)\n",
    "        z = z_mean + eps * tf.exp(z_logvar * 0.5)\n",
    "        \n",
    "        # Combine deterministic and probabilistic\n",
    "        d_o = tf.concat((x_d, z), axis=-1)\n",
    "        \n",
    "        # Decode\n",
    "        logits = self.classifier(d_o)\n",
    "        if return_prob:\n",
    "            p = tf.sigmoid(logits)\n",
    "            return z_mean, z_logvar, p\n",
    "        return z_mean, z_logvar, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_loss(z_mean, z_logvar):\n",
    "    \"\"\"Computes the KL divergence between a unit Gaussian and an arbitrary Gaussian.\"\"\"\n",
    "    \n",
    "    loss = 1 + z_logvar - tf.square(z_mean) - tf.exp(z_logvar)\n",
    "    loss = -0.5 * tf.reduce_sum(loss, axis=-1)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def binary_crossentropy(m_true, logits):\n",
    "    \"\"\"Computes the binary cross-entropy loss.\"\"\"\n",
    "    \n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=m_true, logits=logits)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def softmax_crossentropy(m_true, logits):\n",
    "    \"\"\"Computes the softmax cross-entropy loss.\"\"\"\n",
    "    \n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=m_true, logits=logits)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(batch_size, parameter_settings_rdm, parameter_settings_lfm, \n",
    "                  n_points=None, n_points_min=60, n_points_max=600, to_tensor=True, \n",
    "                  return_params=False):\n",
    "    \"\"\"Generates data from the two models and returns the model indices.\"\"\"\n",
    "    \n",
    "    n = batch_size // 2\n",
    "    # Draw number of trials\n",
    "    if n_points is None:\n",
    "        n_points = np.random.randint(low=n_points_min, high=n_points_max)\n",
    "        \n",
    "    # Generate data\n",
    "    X_batch_rdm, theta_batch_rdm = simulate_diffusion(n, parameter_settings_rdm, n_points=n_points, \n",
    "                                              to_tensor=False, return_indices=False)\n",
    "    X_batch_lfm, theta_batch_lfm = simulate_diffusion(n, parameter_settings_lfm, n_points=n_points, \n",
    "                                              to_tensor=False, return_indices=False)\n",
    "    \n",
    "    # Create indices\n",
    "    X_batch = np.concatenate((X_batch_rdm, X_batch_lfm), axis=0)\n",
    "    theta_batch = np.concatenate((theta_batch_rdm, theta_batch_lfm), axis=0)\n",
    "    m_batch = np.concatenate((np.zeros(n), np.ones(n)))\n",
    "    \n",
    "    \n",
    "    # Shuffle indices and data\n",
    "    shuffle_idx = np.random.permutation(batch_size)\n",
    "    X_batch = X_batch[shuffle_idx]\n",
    "    theta_batch = theta_batch[shuffle_idx]\n",
    "    m_batch = m_batch[shuffle_idx][:, np.newaxis]\n",
    "    \n",
    "    if to_tensor:\n",
    "        X_batch = tf.convert_to_tensor(X_batch, dtype=tf.float32)\n",
    "        m_batch = tf.convert_to_tensor(m_batch, dtype=tf.float32)\n",
    "        if return_params:\n",
    "            theta_batch = tf.convert_to_tensor(theta_batch, dtype=tf.float32)\n",
    "    if return_params:\n",
    "        return X_batch, m_batch, theta_batch\n",
    "    return X_batch, m_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def train_loop_online(model, optimizer, data_generator, iterations, batch_size, \n",
    "                 X_test, m_test, p_bar, beta, transform=None, max_beta=0.2,\n",
    "                 global_step=None, beta_update_every=1000, beta_increment=0.01, clip_value=5.):\n",
    "    \"\"\"\n",
    "    Utility function to perform the # number of training loops given by the itertations argument.\n",
    "    ---------\n",
    "\n",
    "    Arguments:\n",
    "    model           : tf.keras.Model -- the invertible chaoin with an optional summary net\n",
    "                                        both models are jointly trained\n",
    "    optimizer       : tf.train.optimizers.Optimizer -- the optimizer used for backprop\n",
    "    data_generator  : callable -- a function providing batches of X, theta (data, params)\n",
    "    iterations      : int -- the number of training loops to perform\n",
    "    batch_size      : int -- the batch_size used for training\n",
    "    p_bar           : ProgressBar -- an instance for tracking the training progress\n",
    "    clip_value      : float       -- the value used for clipping the gradients\n",
    "    global_step     : tf.EagerVariavle -- a scalar tensor tracking the number of \n",
    "                                            steps and used for learning rate decay  \n",
    "    transform       : callable ot None -- a function to transform X and theta, if given\n",
    "    n_smooth        : int -- a value indicating how many values to use for computing the running ML loss\n",
    "    ----------\n",
    "\n",
    "    Returns:\n",
    "    losses : dict -- a dictionary with the ml_loss and decay\n",
    "    \"\"\"\n",
    "    \n",
    "    for it in range(iterations):\n",
    "\n",
    "        X_batch, m_batch = data_generator(batch_size)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Forward pass\n",
    "            z_mean, z_logvar, m_hat = model(X_batch)\n",
    "\n",
    "            # Compute losses\n",
    "            kl = beta * kl_loss(z_mean, z_logvar)\n",
    "            rec = binary_crossentropy(m_batch, m_hat)\n",
    "            total_loss = kl + rec\n",
    "        \n",
    "        # Compute accuracies\n",
    "        m_hat_train = tf.cast(tf.sigmoid(m_hat) > 0.5, tf.int32).numpy()\n",
    "        _, _, m_hat_test = model(X_test, return_prob=True)\n",
    "        m_hat_test = tf.cast(m_hat_test > 0.5, tf.int32).numpy()\n",
    "        train_acc = np.sum(m_hat_train == m_batch.numpy()) / m_hat_train.shape[0]\n",
    "        test_acc = np.sum(m_hat_test == m_test.numpy()) / m_hat_test.shape[0]\n",
    "\n",
    "        # One step backprop\n",
    "        gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "        if clip_value is not None:\n",
    "            try:\n",
    "                gradients, _ = tf.clip_by_global_norm(gradients, clip_value)\n",
    "            except tf.errors.InvalidArgumentError as e:\n",
    "                print(str(e))\n",
    "                gradients = [tf.clip_by_value(grad, -clip_value, clip_value) for grad in gradients]\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables), global_step=global_step)\n",
    "        \n",
    "        # Increase beta every 500 iterations\n",
    "        if (global_step.numpy() + 1) % beta_update_every == 0:\n",
    "            tf.assign(beta, min(max_beta, beta.numpy() + beta_increment))\n",
    "\n",
    "        # Update p-bar\n",
    "        p_bar.set_postfix_str(\"It: {0}, Loss:{1:.3f},Rec:{2:.3f},KL.:{3:.3f},Train Acc.:{4:.3f},Test Acc.:{5:.3f}\".format(\n",
    "        it, total_loss.numpy(), rec.numpy(), kl.numpy(), train_acc, test_acc))\n",
    "        p_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot latent dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_dim(z_mean, m_test, figsize=(8, 6)):\n",
    "    \"\"\"\n",
    "    Plots the distribution of models in the latent dimension.\n",
    "    Assumes a 2D latent dim.\n",
    "    \"\"\"\n",
    "    \n",
    "    f, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    s = ax.scatter(z_mean[:, 0], z_mean[:, 1], c=m_test.numpy().flatten())\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.axvline(0, color='black')\n",
    "    ax.axhline(0, color='black')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_settings_rdm = OrderedDict([\n",
    "        ('v', [(-5, 5), (-5, 5)]),\n",
    "        ('sv', [(0.0, 2.0)]),\n",
    "        ('zr', [(0.25, 0.75)]),\n",
    "        ('szr', [(0.0, 0.3)]),\n",
    "        ('a', [(0.2, 3.0)]),\n",
    "        ('ndt', [(0.3, 1.5)]),\n",
    "        ('sndt', 0),\n",
    "        ('alpha', 2.0)\n",
    "])\n",
    "\n",
    "parameter_settings_lfm = OrderedDict([\n",
    "        ('v', [(-5, 5), (-5, 5)]),\n",
    "        ('sv', 0),\n",
    "        ('zr', [(0.25, 0.75)]),\n",
    "        ('szr', 0),\n",
    "        ('a', [(0.2, 3.0)]),\n",
    "        ('ndt', [(0.3, 1.5)]),\n",
    "        ('sndt', 0),\n",
    "        ('alpha', [(1.0, 2.0)])\n",
    "])\n",
    "nn_params = dict(n_cond=2, inv_h_dim=128, n_equiv=4, z_dim=64, summary_dim=64, cls_dim=128, n_dense_cls=2)\n",
    "n_post_samples = 1000\n",
    "n_trials = 500\n",
    "n_test = 300\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "iterations_per_epoch = 1000\n",
    "clip_value = 5.\n",
    "global_step = tfe.Variable(0, dtype=tf.int32)\n",
    "beta = tfe.Variable(0.0, dtype=tf.float32)\n",
    "data_gen = partial(generate_data, parameter_settings_rdm=parameter_settings_rdm, \n",
    "                   parameter_settings_lfm=parameter_settings_lfm, n_points=n_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, m_test, theta_test = data_gen(n_test, return_params=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelSelector(**nn_params)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot latent dim of untrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_means = model(X_test)[0]\n",
    "# plot_latent_dim(z_means, m_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing from scratch.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.Checkpoint(step=global_step, optimizer=optimizer, net=model)\n",
    "manager = tf.train.CheckpointManager(checkpoint, './checkpoints/rdm_lfm', max_to_keep=5)\n",
    "checkpoint.restore(manager.latest_checkpoint)\n",
    "if manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272d593ce3b84896b3e5b38eefde8f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training epoch 1', max=1000, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of beta:  0.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9de2abf3164b2088fd523107351df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training epoch 2', max=1000, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of beta:  0.1\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "680e822b48b043d3af56438c587b6fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training epoch 3', max=1000, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of beta:  0.2\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed31ad6858554df2b1caadf4cd12cbc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training epoch 4', max=1000, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of beta:  0.3\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe64e8afade4e07b5dda8ca67828d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training epoch 5', max=1000, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of beta:  0.4\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edecdeabd1bb42829f85beb68c94735c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training epoch 6', max=1000, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of beta:  0.5\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\TensorFlowGpu\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_management.py:624: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a41c963c184ca8b02196d373520608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training epoch 7', max=1000, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of beta:  0.6\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537466332b19402883909728e4a46f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training epoch 8', max=1000, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of beta:  0.70000005\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-b74eb43a9585>\u001b[0m in \u001b[0;36mtrain_loop_online\u001b[1;34m(model, optimizer, data_generator, iterations, batch_size, X_test, m_test, p_bar, beta, transform, max_beta, global_step, beta_update_every, beta_increment, clip_value)\u001b[0m\n\u001b[0;32m     54\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_by_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mclip_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip_value\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mgrad\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[1;31m# Increase beta every 500 iterations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\TensorFlowGpu\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, global_step, name)\u001b[0m\n\u001b[0;32m    610\u001b[0m           \u001b[0mscope_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"update_\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 612\u001b[1;33m           \u001b[0mupdate_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    613\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[0mapply_updates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_finish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate_ops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\TensorFlowGpu\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mupdate_op\u001b[1;34m(self, optimizer, g)\u001b[0m\n\u001b[0;32m    169\u001b[0m       return optimizer._resource_apply_sparse_duplicate_indices(\n\u001b[0;32m    170\u001b[0m           g.values, self._v, g.indices)\n\u001b[1;32m--> 171\u001b[1;33m     \u001b[0mupdate_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_resource_apply_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstraint\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\TensorFlowGpu\\lib\\site-packages\\tensorflow\\python\\training\\adam.py\u001b[0m in \u001b[0;36m_resource_apply_dense\u001b[1;34m(self, grad, var)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_beta2_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_epsilon_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         grad, use_locking=self._use_locking)\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_apply_sparse_shared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscatter_add\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\TensorFlowGpu\\lib\\site-packages\\tensorflow\\python\\training\\gen_training_ops.py\u001b[0m in \u001b[0;36mresource_apply_adam\u001b[1;34m(var, m, v, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad, use_locking, use_nesterov, name)\u001b[0m\n\u001b[0;32m   1272\u001b[0m         \u001b[1;34m\"ResourceApplyAdam\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1273\u001b[0m         \u001b[0mbeta1_power\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta2_power\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1274\u001b[1;33m         \"use_locking\", use_locking, \"use_nesterov\", use_nesterov)\n\u001b[0m\u001b[0;32m   1275\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for ep in range(1, epochs+1):\n",
    "    with tqdm(total=iterations_per_epoch, desc='Training epoch {}'.format(ep)) as p_bar:\n",
    "        print('Value of beta: ', beta.numpy())\n",
    "        losses = train_loop_online(model, optimizer, data_gen, iterations_per_epoch, \n",
    "                            batch_size, X_test, m_test, p_bar, beta, clip_value=clip_value, \n",
    "                            global_step=global_step, beta_update_every=1000, beta_increment=0.1, max_beta=1.0)\n",
    "#         z_mean, _, _ = model(X_test, return_prob=True)\n",
    "#         plot_latent_dim(z_mean, m_test)\n",
    "        manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z_mean, z_var, _ = model(X_test, return_prob=True)\n",
    "plot_latent_dim(z_mean, m_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = model.sample(X_test, n_post_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean, z_var, m_hat = model(X_test, return_prob=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent(z_mean, m_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_uncertain = np.logical_and(samples.numpy().mean(1) > .3, samples.numpy().mean(1) < .7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_wrong = (m_hat.numpy() > 0.5).astype(np.int32) != m_test.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(index_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1, 5, figsize=(16, 3))\n",
    "axarr = axarr.flat\n",
    "\n",
    "sns.distplot(samples[89], hist=True, kde=True, ax=axarr[0])\n",
    "sns.distplot(1 - samples[89], hist=True, kde=True, ax=axarr[0])\n",
    "\n",
    "sns.distplot(samples[129], hist=True, kde=True, ax=axarr[1])\n",
    "sns.distplot(1 - samples[129], hist=True, kde=True, ax=axarr[1])\n",
    "\n",
    "sns.distplot(samples[133], hist=True, kde=True, ax=axarr[2])\n",
    "sns.distplot(1 - samples[133], hist=True, kde=True, ax=axarr[2])\n",
    "\n",
    "sns.distplot(samples[164], hist=True, kde=True, ax=axarr[3])\n",
    "sns.distplot(1 - samples[164], hist=True, kde=True, ax=axarr[3])\n",
    "\n",
    "sns.distplot(samples[293], hist=True, kde=True, ax=axarr[4])\n",
    "sns.distplot(1 - samples[293], hist=True, kde=True, ax=axarr[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1, 5, figsize=(16, 3))\n",
    "axarr = axarr.flat\n",
    "\n",
    "sns.distplot(samples[0], hist=True, kde=True, ax=axarr[0])\n",
    "sns.distplot(1 - samples[0], hist=True, kde=True, ax=axarr[0])\n",
    "\n",
    "sns.distplot(samples[1], hist=True, kde=True, ax=axarr[1])\n",
    "sns.distplot(1 - samples[1], hist=True, kde=True, ax=axarr[1])\n",
    "\n",
    "sns.distplot(samples[2], hist=True, kde=True, ax=axarr[2])\n",
    "sns.distplot(1 - samples[2], hist=True, kde=True, ax=axarr[2])\n",
    "\n",
    "sns.distplot(samples[3], hist=True, kde=True, ax=axarr[3])\n",
    "sns.distplot(1 - samples[3], hist=True, kde=True, ax=axarr[3])\n",
    "\n",
    "sns.distplot(samples[4], hist=True, kde=True, ax=axarr[4])\n",
    "sns.distplot(1 - samples[4], hist=True, kde=True, ax=axarr[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_hat.numpy()[index_wrong]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_hat.numpy()[index_uncertain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong = X_test.numpy()[index_wrong.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_wrong = z_mean.numpy()[index_wrong.flatten()]\n",
    "stds_wrong = np.exp(z_var.numpy()[index_wrong.flatten()] * 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = np.random.normal(means_wrong[0, 0], stds_wrong[0, 0], 1000)\n",
    "z2 = np.random.normal(means_wrong[0, 1], stds_wrong[0, 1], 1000)\n",
    "sns.jointplot(z1, z2, kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = np.random.normal(means_wrong[1, 0], stds_wrong[1, 0], 1000)\n",
    "z2 = np.random.normal(means_wrong[1, 1], stds_wrong[1, 1], 1000)\n",
    "sns.jointplot(z1, z2, kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = np.random.normal(means_wrong[2, 0], stds_wrong[2, 0], 1000)\n",
    "z2 = np.random.normal(means_wrong[2, 1], stds_wrong[2, 1], 1000)\n",
    "sns.jointplot(z1, z2, kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(m_test.numpy().flatten(), m_hat.numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
